# Implementing conversation with RAG using Pre-trained Falcon-7b

Implements RAG and Conversation using the ConversationalRetrievalChain of langchain. <br />
Loads <data_file>.md file and vectorizes and stores data in FIASS which is later used for RAG. <br />
Maintains chat history to ensure conversation. <br />
<br/>
I have attempted to build on the original notebook by curiousily who has coded for a conversational chatbot using pre-trained Falcon-7b. The notebook can be found here: <br />
<br/>
https://github.com/curiousily/Get-Things-Done-with-Prompt-Engineering-and-LangChain/blob/master/11.chatbot-with-local-llm-falcon-7b.ipynb <br />
<br/>

However, with large context, RAG needs to be implemented not only to improve inference time but also to improve accuracy. I have used ConversationalRetrievalChain on a dummy data generated by ChatGPT. "all-MiniLM-L6-v2" from HuggingFaceEmbeddings is used to create data embeddings, which are then split and stored using FIASS. "tiiuae/falcon-7b-instruct" is used as LLM, therefore no OpenAI or Gemini API is required.
